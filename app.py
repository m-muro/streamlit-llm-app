import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

load_dotenv()

# å°‚é–€å®¶ã®å®šç¾©
EXPERTS = {
    "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢": "ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«ã¤ã„ã¦å°‚é–€çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
    "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ": "ã‚ãªãŸã¯ç†Ÿç·´ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿åˆ†æã€æ©Ÿæ¢°å­¦ç¿’ã€çµ±è¨ˆå­¦ã«ã¤ã„ã¦å°‚é–€çš„ãªçŸ¥è¦‹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
    "ãƒ“ã‚¸ãƒã‚¹ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ": "ã‚ãªãŸã¯å„ªç§€ãªãƒ“ã‚¸ãƒã‚¹ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚çµŒå–¶æˆ¦ç•¥ã€æ¥­å‹™æ”¹å–„ã€å¸‚å ´åˆ†æã«ã¤ã„ã¦å°‚é–€çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
}

def get_llm_response(user_input: str, expert_type: str) -> str:
    """
    LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•°
    
    Args:
        user_input (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        expert_type (str): é¸æŠã•ã‚ŒãŸå°‚é–€å®¶ã®ã‚¿ã‚¤ãƒ—
    
    Returns:
        str: LLMã‹ã‚‰ã®å›ç­”
    """
    # ChatOpenAIã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
    system_message = SystemMessage(content=EXPERTS[expert_type])
    human_message = HumanMessage(content=user_input)
    
    # LLMã«å•ã„åˆã‚ã›
    response = llm.invoke([system_message, human_message])
    
    return response.content

# Streamlitã‚¢ãƒ—ãƒªã®ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†
def main():
    st.title("ğŸ¤– AIå°‚é–€å®¶ãƒãƒ£ãƒƒãƒˆ")
    
    # ã‚¢ãƒ—ãƒªã®æ¦‚è¦
    st.markdown("""
    ## ğŸ“– æ¦‚è¦
    ã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã€æ§˜ã€…ãªåˆ†é‡ã®å°‚é–€å®¶ã¨ã—ã¦AIã«è³ªå•ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
    
    ## ğŸ¯ ä½¿ã„æ–¹
    1. **å°‚é–€å®¶ã‚’é¸æŠ**: ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã‹ã‚‰ç›¸è«‡ã—ãŸã„å°‚é–€å®¶ã®ã‚¿ã‚¤ãƒ—ã‚’é¸æŠã—ã¦ãã ã•ã„
    2. **è³ªå•ã‚’å…¥åŠ›**: ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã«è³ªå•ã‚„ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„
    3. **é€ä¿¡**: ã€Œå›ç­”ã‚’å–å¾—ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€é¸æŠã—ãŸå°‚é–€å®¶ã®è¦–ç‚¹ã§å›ç­”ãŒè¡¨ç¤ºã•ã‚Œã¾ã™
    """)
    
    st.divider()
    
    # å°‚é–€å®¶ã®é¸æŠ
    st.subheader("ğŸ‘¤ å°‚é–€å®¶ã‚’é¸æŠ")
    expert_type = st.radio(
        "ã©ã®åˆ†é‡ã®å°‚é–€å®¶ã«ç›¸è«‡ã—ã¾ã™ã‹ï¼Ÿ",
        options=list(EXPERTS.keys()),
        index=0
    )
    
    # é¸æŠã•ã‚ŒãŸå°‚é–€å®¶ã®èª¬æ˜ã‚’è¡¨ç¤º
    st.info(f"**é¸æŠä¸­**: {expert_type}\n\n{EXPERTS[expert_type]}")
    
    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    st.subheader("ğŸ’¬ è³ªå•ã‚’å…¥åŠ›")
    user_input = st.text_area(
        "è³ªå•ã‚„ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        height=150,
        placeholder="ä¾‹: Pythonã§ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’å§‹ã‚ã‚‹ã«ã¯ä½•ã‹ã‚‰å­¦ã¹ã°ã„ã„ã§ã™ã‹ï¼Ÿ"
    )
    
    # é€ä¿¡ãƒœã‚¿ãƒ³
    if st.button("ğŸš€ å›ç­”ã‚’å–å¾—", type="primary"):
        if user_input.strip():
            with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                try:
                    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—
                    response = get_llm_response(user_input, expert_type)
                    
                    # å›ç­”ã‚’è¡¨ç¤º
                    st.subheader("âœ¨ å›ç­”")
                    st.success(response)
                    
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        else:
            st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()